#!/bin/bash
# Download all CAGVault RAG-optimized models
# Run this script to pull all supported models

echo "üöÄ Downloading CAGVault RAG Models"
echo "===================================="
echo ""
echo "‚ö†Ô∏è  This will download ~200GB+ of models"
echo "‚ö†Ô∏è  Estimated time: 2-4 hours depending on your connection"
echo "‚ö†Ô∏è  Press Ctrl+C to cancel"
echo ""
read -p "Continue? (y/n) " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    exit 1
fi

echo ""
echo "üì¶ Downloading lightweight models (8GB RAM)..."
echo "----------------------------------------------"
ollama pull llama3.1:8b
ollama pull mistral-small-latest

echo ""
echo "üì¶ Downloading medium models (16GB RAM)..."
echo "-------------------------------------------"
ollama pull hf.co/unsloth/Qwen3-14B-GGUF:Q4_K_XL
ollama pull phi4:latest

echo ""
echo "üì¶ Downloading large models (32GB+ RAM)..."
echo "-------------------------------------------"
ollama pull gemma2:27b
ollama pull llama3.3:70b
ollama pull mistral-large-latest
ollama pull command-r-plus:latest

echo ""
echo "üì¶ Downloading state-of-the-art models (64GB+ RAM)..."
echo "-------------------------------------------------------"
ollama pull deepseek-ai/DeepSeek-V3
ollama pull deepseek-ai/DeepSeek-R1

echo ""
echo "‚úÖ All models downloaded successfully!"
echo ""
echo "To use a model, either:"
echo "  1. Select it from the UI sidebar (Model Settings)"
echo "  2. Edit config.py and change Config.MODEL"
echo ""
echo "Installed models:"
ollama list
